"""
Benchmarking module for evaluating AI models for trading strategy advisors.
Provides functionality to compare different models against standardized tests.
"""

import time
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Callable, Tuple, Union
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from app.utils.metrics import EvaluationMetrics


class ModelBenchmark:
    """
    A class for benchmarking different AI models against standardized tests
    and evaluation criteria for trading strategy advisors.
    """
    
    def __init__(self, models: Dict[str, Any], test_cases: List[Dict[str, Any]]):
        """
        Initialize the benchmarking framework.
        
        Args:
            models: Dictionary of model instances to benchmark, with model names as keys
            test_cases: List of test cases with input data and expected outputs
        """
        self.models = models
        self.test_cases = test_cases
        self.metrics = EvaluationMetrics()
        self.results = {}
    
    def run_benchmark(self) -> Dict[str, Any]:
        """
        Run benchmark tests on all models using the provided test cases.
        
        Returns:
            Dictionary with benchmark results for each model
        """
        results = {}
        
        for model_name, model in self.models.items():
            print(f"Benchmarking model: {model_name}")
            model_results = {
                "text_quality": [],
                "pattern_recognition": [],
                "strategy_performance": [],
                "latency": []
            }
            
            for test_case in self.test_cases:
                # Measure latency
                start_time = time.time()
                model_output = self._run_model(model, test_case["input"])
                end_time = time.time()
                latency = self.metrics.measure_latency(start_time, end_time)
                
                # Evaluate text quality
                text_metrics = self._evaluate_text_quality(
                    model_output.get("explanation", ""),
                    test_case["expected"]["explanation"]
                )
                
                # Evaluate pattern recognition
                pattern_metrics = self.metrics.calculate_pattern_recognition_metrics(
                    test_case["expected"]["patterns"],
                    model_output.get("patterns", [])
                )
                
                # Evaluate trading strategy performance if backtesting data is available
                if "backtest_returns" in test_case["expected"] and "backtest_returns" in model_output:
                    strategy_metrics = self.metrics.calculate_backtest_metrics(
                        model_output["backtest_returns"]
                    )
                else:
                    strategy_metrics = {
                        "sharpe_ratio": 0.0,
                        "max_drawdown": 0.0,
                        "total_return": 0.0,
                        "volatility": 0.0
                    }
                
                # Append results
                model_results["text_quality"].append(text_metrics)
                model_results["pattern_recognition"].append(pattern_metrics)
                model_results["strategy_performance"].append(strategy_metrics)
                model_results["latency"].append(latency)
            
            # Calculate aggregate results
            results[model_name] = self._aggregate_results(model_results)
        
        self.results = results
        return results
    
    def _run_model(self, model: Any, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run a model on the input data from a test case.
        
        Args:
            model: Model instance
            input_data: Input data for the model
            
        Returns:
            Model output as a dictionary
        """
        try:
            if hasattr(model, "analyze_market") and callable(model.analyze_market):
                return model.analyze_market(input_data)
            elif callable(model):
                return model(input_data)
            else:
                raise ValueError(f"Model does not have a compatible interface")
        except Exception as e:
            print(f"Error running model: {e}")
            return {}
    
    def _evaluate_text_quality(self, 
                              generated_text: str, 
                              reference_text: Union[str, List[str]]) -> Dict[str, float]:
        """
        Evaluate the quality of generated text against reference text.
        
        Args:
            generated_text: Text generated by the model
            reference_text: Reference text or list of reference texts
            
        Returns:
            Dictionary with text quality metrics
        """
        if isinstance(reference_text, str):
            reference_texts = [reference_text]
        else:
            reference_texts = reference_text
        
        bleu_score = self.metrics.calculate_bleu(reference_texts, generated_text)
        
        # For ROUGE, we use the first reference if multiple are provided
        rouge_scores = self.metrics.calculate_rouge(reference_texts[0], generated_text)
        
        meteor_score = self.metrics.calculate_meteor(reference_texts, generated_text)
        
        return {
            "bleu": bleu_score,
            **rouge_scores,
            "meteor": meteor_score
        }
    
    def _aggregate_results(self, model_results: Dict[str, List[Dict[str, float]]]) -> Dict[str, Any]:
        """
        Aggregate results across all test cases for a model.
        
        Args:
            model_results: Results for each test case
            
        Returns:
            Aggregated results as a dictionary
        """
        aggregated = {
            "text_quality": {},
            "pattern_recognition": {},
            "strategy_performance": {},
            "latency": {"mean": 0.0, "std": 0.0, "min": 0.0, "max": 0.0}
        }
        
        # Aggregate text quality metrics
        if model_results["text_quality"]:
            text_metrics = {}
            for metric in model_results["text_quality"][0].keys():
                values = [result[metric] for result in model_results["text_quality"]]
                text_metrics[metric] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": np.min(values),
                    "max": np.max(values)
                }
            aggregated["text_quality"] = text_metrics
        
        # Aggregate pattern recognition metrics
        if model_results["pattern_recognition"]:
            pattern_metrics = {}
            for metric in model_results["pattern_recognition"][0].keys():
                values = [result[metric] for result in model_results["pattern_recognition"]]
                pattern_metrics[metric] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": np.min(values),
                    "max": np.max(values)
                }
            aggregated["pattern_recognition"] = pattern_metrics
        
        # Aggregate strategy performance metrics
        if model_results["strategy_performance"]:
            strategy_metrics = {}
            for metric in model_results["strategy_performance"][0].keys():
                values = [result[metric] for result in model_results["strategy_performance"]]
                strategy_metrics[metric] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": np.min(values),
                    "max": np.max(values)
                }
            aggregated["strategy_performance"] = strategy_metrics
        
        # Aggregate latency
        if model_results["latency"]:
            latency_values = model_results["latency"]
            aggregated["latency"] = {
                "mean": np.mean(latency_values),
                "std": np.std(latency_values),
                "min": np.min(latency_values),
                "max": np.max(latency_values)
            }
        
        return aggregated
    
    def generate_report(self, output_path: str = None) -> str:
        """
        Generate a benchmark report with comparison between models.
        
        Args:
            output_path: Path to save the report (optional)
            
        Returns:
            Report as a JSON string
        """
        if not self.results:
            raise ValueError("No benchmark results available. Run benchmark first.")
        
        # Prepare report structure
        report = {
            "timestamp": datetime.now().isoformat(),
            "models_benchmarked": list(self.results.keys()),
            "num_test_cases": len(self.test_cases),
            "results": self.results,
            "comparative_analysis": self._generate_comparative_analysis()
        }
        
        # Convert report to JSON
        report_json = json.dumps(report, indent=2)
        
        # Save report if output path is provided
        if output_path:
            with open(output_path, 'w') as f:
                f.write(report_json)
            print(f"Benchmark report saved to {output_path}")
        
        return report_json
    
    def _generate_comparative_analysis(self) -> Dict[str, Any]:
        """
        Generate comparative analysis of models.
        
        Returns:
            Dictionary with comparative metrics
        """
        if not self.results:
            return {}
        
        comparative = {
            "text_quality": {},
            "pattern_recognition": {},
            "strategy_performance": {},
            "latency": {},
            "overall_ranking": {}
        }
        
        # Prepare metric names for each category
        text_metrics = list(next(iter(self.results.values()))["text_quality"].keys())
        pattern_metrics = list(next(iter(self.results.values()))["pattern_recognition"].keys())
        strategy_metrics = list(next(iter(self.results.values()))["strategy_performance"].keys())
        
        # Compare text quality
        for metric in text_metrics:
            model_values = {name: results["text_quality"][metric]["mean"] 
                           for name, results in self.results.items()}
            comparative["text_quality"][metric] = model_values
        
        # Compare pattern recognition
        for metric in pattern_metrics:
            model_values = {name: results["pattern_recognition"][metric]["mean"] 
                           for name, results in self.results.items()}
            comparative["pattern_recognition"][metric] = model_values
        
        # Compare strategy performance
        for metric in strategy_metrics:
            model_values = {name: results["strategy_performance"][metric]["mean"] 
                           for name, results in self.results.items()}
            comparative["strategy_performance"][metric] = model_values
        
        # Compare latency
        model_latencies = {name: results["latency"]["mean"] 
                          for name, results in self.results.items()}
        comparative["latency"] = model_latencies
        
        # Calculate overall ranking (simple average of normalized metrics)
        # This is a basic approach and can be refined based on specific requirements
        model_scores = {name: 0.0 for name in self.results.keys()}
        
        # Add text quality scores
        for metric in text_metrics:
            values = [self.results[name]["text_quality"][metric]["mean"] for name in self.results.keys()]
            max_val = max(values) if values and max(values) > 0 else 1.0
            for name in self.results.keys():
                model_scores[name] += self.results[name]["text_quality"][metric]["mean"] / max_val
        
        # Add pattern recognition scores
        for metric in pattern_metrics:
            values = [self.results[name]["pattern_recognition"][metric]["mean"] for name in self.results.keys()]
            max_val = max(values) if values and max(values) > 0 else 1.0
            for name in self.results.keys():
                model_scores[name] += self.results[name]["pattern_recognition"][metric]["mean"] / max_val
        
        # Add strategy performance scores (only Sharpe ratio and total return)
        for metric in ["sharpe_ratio", "total_return"]:
            if metric in strategy_metrics:
                values = [self.results[name]["strategy_performance"][metric]["mean"] for name in self.results.keys()]
                max_val = max(values) if values and max(values) > 0 else 1.0
                for name in self.results.keys():
                    model_scores[name] += self.results[name]["strategy_performance"][metric]["mean"] / max_val
        
        # Subtract latency (lower is better)
        latency_values = [self.results[name]["latency"]["mean"] for name in self.results.keys()]
        max_latency = max(latency_values) if latency_values and max(latency_values) > 0 else 1.0
        for name in self.results.keys():
            # Invert latency so lower values get higher scores
            model_scores[name] += (1 - (self.results[name]["latency"]["mean"] / max_latency))
        
        # Normalize by number of metrics
        total_metrics = len(text_metrics) + len(pattern_metrics) + 2 + 1  # text + pattern + sharpe & return + latency
        for name in model_scores:
            model_scores[name] /= total_metrics
        
        comparative["overall_ranking"] = model_scores
        
        return comparative
    
    def visualize_results(self, output_path: str = None) -> None:
        """
        Generate visualizations of benchmark results.
        
        Args:
            output_path: Directory to save visualizations (optional)
        """
        if not self.results:
            raise ValueError("No benchmark results available. Run benchmark first.")
        
        # Set up plot style
        plt.style.use('seaborn-v0_8-darkgrid')
        
        # Plot text quality metrics
        self._plot_metrics(
            metrics_category="text_quality",
            title="Text Quality Metrics Comparison",
            ylabel="Score",
            output_path=output_path
        )
        
        # Plot pattern recognition metrics
        self._plot_metrics(
            metrics_category="pattern_recognition",
            title="Pattern Recognition Metrics Comparison",
            ylabel="Score",
            output_path=output_path
        )
        
        # Plot strategy performance metrics
        self._plot_metrics(
            metrics_category="strategy_performance",
            title="Trading Strategy Performance Metrics",
            ylabel="Score",
            output_path=output_path
        )
        
        # Plot latency
        model_names = list(self.results.keys())
        latencies = [self.results[model]["latency"]["mean"] for model in model_names]
        
        plt.figure(figsize=(10, 6))
        sns.barplot(x=model_names, y=latencies)
        plt.title("Model Response Latency Comparison")
        plt.ylabel("Latency (seconds)")
        plt.xlabel("Model")
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        if output_path:
            plt.savefig(f"{output_path}/latency_comparison.png", dpi=300)
        plt.close()
        
        # Plot overall ranking
        comparative = self._generate_comparative_analysis()
        model_names = list(comparative["overall_ranking"].keys())
        scores = list(comparative["overall_ranking"].values())
        
        plt.figure(figsize=(10, 6))
        sns.barplot(x=model_names, y=scores)
        plt.title("Overall Model Ranking")
        plt.ylabel("Normalized Score")
        plt.xlabel("Model")
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        if output_path:
            plt.savefig(f"{output_path}/overall_ranking.png", dpi=300)
        plt.close()
    
    def _plot_metrics(self, metrics_category: str, title: str, ylabel: str, output_path: str = None) -> None:
        """
        Plot metrics for a specific category.
        
        Args:
            metrics_category: Category of metrics to plot
            title: Plot title
            ylabel: Y-axis label
            output_path: Directory to save visualization (optional)
        """
        # Get all metrics in this category
        metrics = list(next(iter(self.results.values()))[metrics_category].keys())
        model_names = list(self.results.keys())
        
        # Create DataFrame for plotting
        data = []
        for model_name in model_names:
            for metric in metrics:
                mean_value = self.results[model_name][metrics_category][metric]["mean"]
                std_value = self.results[model_name][metrics_category][metric]["std"]
                data.append({
                    "Model": model_name,
                    "Metric": metric,
                    "Value": mean_value,
                    "Std": std_value
                })
        
        df = pd.DataFrame(data)
        
        # Plot
        plt.figure(figsize=(12, 8))
        g = sns.barplot(x="Metric", y="Value", hue="Model", data=df)
        plt.title(title)
        plt.ylabel(ylabel)
        plt.xlabel("Metric")
        plt.xticks(rotation=45)
        plt.legend(title="Model")
        plt.tight_layout()
        
        if output_path:
            plt.savefig(f"{output_path}/{metrics_category}_comparison.png", dpi=300)
        plt.close()